{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356f7de8-2618-4376-9692-4a5fe5330f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingest Customers Data Into Bronze Layer With Autoloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caaa7ef-9d20-4381-a28d-3ca87a8a87d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3278043-a780-4ede-8d88-1824112c638e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_unique_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True),\n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    StructField(\"source_file_timestamp\", TimestampType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d719591c-86dd-4fe7-8427-eef232fdd8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Stream Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab36ebf-a2b7-40bf-b57f-bd522613c764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"/Volumes/mycatalog/olist_ecommerce_bronze/checkpoints/customers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86b943e-915f-433a-8345-44f170b6d49c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(schema)\\\n",
    "    .format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint}/schema\")\\\n",
    "    .load(\"/Volumes/mycatalog/olist_ecommerce/olist_landing/customers\")\\\n",
    "    .selectExpr(\"*\", \"_metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db80a81-4b25-4f72-9e24-c8d2b3c6ab06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"source_file\", df._metadata.file_name) \\\n",
    "    .withColumn(\n",
    "        \"source_file_timestamp\",\n",
    "        df._metadata.file_modification_time.cast(\"timestamp\")\n",
    "    ) \\\n",
    "    .drop(\"_metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae96a8b5-bbce-4001-a121-df124821b766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Stream Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b63c44-bc35-48e4-9478-058217c536e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint}/_checkpoint\")\\\n",
    "    .toTable(\"mycatalog.olist_ecommerce_bronze.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5e87b2-d898-4c0b-9892-49615d04e233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b5db1a-06dd-418b-a287-9b66350a8a45",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770932812424}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SELECT * FROM mycatalog.olist_ecommerce_bronze.customers LIMIT 5\n",
    "SELECT * FROM mycatalog.olist_ecommerce_bronze.customers where customer_id = '5ea2072bf6d8282cf452c471506c54a3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3d3908-e045-4ef4-b120-ce01408c9891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Handling in Structured Streaming (Auto Loader)\n",
    "\n",
    "1️⃣ When Schema Is Not Explicitly Defined\n",
    "\n",
    "If you do not provide a schema while reading data using spark.readStream,\n",
    "Auto Loader infers the schema automatically.\n",
    "\n",
    "When new columns appear in incoming files, they are automatically added to the schema (default behavior).\n",
    "\n",
    "2️⃣ When Schema Is Explicitly Defined\n",
    "\n",
    "If you manually define a schema using .schema(...),\n",
    "Spark enforces that schema strictly.\n",
    "\n",
    "Any new or unexpected columns in the input files are silently dropped.\n",
    "\n",
    "The stream continues without failure.\n",
    "\n",
    "3️⃣ When Schema Enforcement + Drift Capture Is Required\n",
    "\n",
    "If you want to:\n",
    "\n",
    "- Enforce a defined schema\n",
    "- Prevent pipeline failures\n",
    "- Capture unexpected/new columns\n",
    "\n",
    "You should enable **schema evolution** along with **mergeSchema**.\n",
    "\n",
    "In this ingestion, schema evolution was configured to capture unexpected columns in the _rescued_data column, allowing the pipeline to continue without failure.\n",
    "\n",
    "### Schema Evolution Modes in Auto Loader\n",
    "\n",
    "Auto Loader supports three schema evolution modes:\n",
    "\n",
    "- **addNewColumns** (default)\n",
    "→ Automatically adds new columns to the table schema.\n",
    "\n",
    "- rescue\n",
    "→ **Captures** new/unexpected columns in a special _rescued_data column instead of modifying the table schema.\n",
    "\n",
    "- **failOnNewColumns**\n",
    "→ Fails the stream immediately if new columns are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e32b0c-f704-444b-9215-b864563434f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3912443640424710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
